{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data\n",
    "data = pd.read_csv('D:\\Pratik\\Data Science\\Ecommer Reviews\\Womens Clothing E-Commerce Reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout Unwanted Features\n",
    "data.drop(['Unnamed: 0','Clothing ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns names\n",
    "data.rename(columns={'Review Text': 'text',\n",
    "                    'Positive Feedback Count': 'feedback_count',\n",
    "                    'Division Name': 'Division', 'Department Name': 'Department',\n",
    "                    'Class Name':'Class', 'Recommended IND': 'Recommended'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our New Cloumns Names\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look into the missing vlaues\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing out every row in the DF which contains missing data\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This histgram below represent that the 40 Yrs Old ladies are the main customers of the Site\n",
    "sns.histplot(data=data, x='Age', kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The majority of site visitors are more likely to give 5 rates for each product\n",
    "sns.histplot(data=data, x='Rating', kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layering class on the top of whole ratings comparing to other classes\n",
    "sns.set_style('white')\n",
    "sns.barplot(x=data['Rating'], y=data['Class'],palette=\"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sweaters are more preferable for those whom are above 40's where Casual bottoms attrcats 20's as this barplot shows below\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.color_palette(\"flare\", as_cmap=False)\n",
    "sns.barplot(x=data['Age'], y=data['Class'],palette=\"Reds_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot shows the ditribution of positive ratings\n",
    "sns.displot(data=data,\n",
    "           x='Rating', hue=\"Class\",\n",
    "    kind=\"kde\", height=6,\n",
    "    multiple=\"fill\", clip=(0, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This boxplot below determine the mean age according to each department\n",
    "sns.boxplot(x=data['Department'], y=data['Age'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Piechart visualize that the Tops department has gotten the most amounts of ratings, where Trends got the low amount of ratings\n",
    "data.groupby(['Department']).sum().plot(kind='pie', subplots=False, shadow = False,startangle=90,figsize=(15,10), y='Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which one is highly Recommended\n",
    "sns.countplot(data=data, y='Class', hue='Recommended',  palette = \"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, y='Department', hue='Recommended' ,palette = \"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, y='Division', hue='Recommended', palette = \"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with creating word cloud\n",
    "from wordcloud import WordCloud\n",
    "text = \" \".join(cat.split()[1] for cat in data.text)\n",
    "# Creating word_cloud with text as argument in .generate() method\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries for NLP \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['all_text'] = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Function\n",
    "def tokenize(column):\n",
    "    # Tokenize a Pandas dataframe columns and returns a list of tokens.\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Tokenization to Reviews column\n",
    "data['tokenized'] = data.apply(lambda x: tokenize(x['all_text']), axis=1)\n",
    "data[['text', 'tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stiopewords\n",
    "def remove_stopwords(tokenized_column):\n",
    "    # this fuction will retun a list of tokens with English stopwords removed\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return [word for word in tokenized_column if not word in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Stopwords removal\n",
    "data['stopwords_removed'] = data.apply(lambda x: remove_stopwords(x['tokenized']), axis=1)\n",
    "data[['text', 'stopwords_removed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "def apply_stemming(tokenized_column):\n",
    "    # this function will return a list of tokens with PorterStemming applied\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokenized_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['porter_stemmed'] = data.apply(lambda x: apply_stemming(x['stopwords_removed']), axis=1)\n",
    "data[['text', 'porter_stemmed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rejoin words\n",
    "def rejoin_words(tokenized_column):\n",
    "    # this fuction will rejoin the tokenized words list into a single string\n",
    "    return (\" \".join(tokenized_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rejoined'] = data.apply(lambda x: rejoin_words(x['porter_stemmed']), axis=1)\n",
    "data[['text', 'rejoined']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(cat.split()[1] for cat in data.rejoined)\n",
    "# Creating word_cloud with text as argument in .generate() method\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data into trainng and testing data:- \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['rejoined'],\n",
    "                                                   data['Recommended'], test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train first entry :\\n\\n', X_train.iloc[1])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the countVectorizer to the training data:-\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names()[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the document in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Support vector machine classifier\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear')\n",
    "# train the classifier: \n",
    "clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "# predict the transformed test documents\n",
    "predictions = clf.predict(vect.transform(X_test))\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Accuracy score: ', round(accuracy_score(y_test, predictions,2)))\n",
    "print('F1_score: ', round(f1_score(y_test, predictions,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df = 5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('accuracy :', accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d341f0111f7beede37191a6b3b160b44ae2dbf445f138238f17ca27644bd15b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
